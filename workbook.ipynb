{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from copy import deepcopy\n",
    "from importlib import reload\n",
    "from collections import namedtuple\n",
    "import traceback\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import argparse\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import models\n",
    "reload(models)\n",
    "import models\n",
    "import utils\n",
    "reload(utils)\n",
    "import utils\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "from torch.optim import Adam\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, BatchSizeFinder\n",
    "\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "#cmd command: tensorboard --logdir=lightning_logs/\n",
    "\n",
    "random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = 'data/tiny-imagenet-200'\n",
    "TRAIN_DIR = os.path.join(DATA_DIR, 'train') \n",
    "VALID_DIR = os.path.join(DATA_DIR, 'val')\n",
    "VALID_IMG_DIR = os.path.join(VALID_DIR, 'images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_transform_pretrain = transforms.Compose([\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                            std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = utils.generate_dataloader(TRAIN_DIR, \"train\", categories=categories, \n",
    "                                  transform=preprocess_transform_pretrain, batch_size=256, unseen=False)\n",
    "val_seen_loader = utils.generate_dataloader(VALID_IMG_DIR, \"val_seen\", categories=categories, \n",
    "                                  transform=preprocess_transform_pretrain, batch_size=256, unseen=False)\n",
    "val_unseen_loader = utils.generate_dataloader(VALID_IMG_DIR, \"val_unseen\", categories=categories, \n",
    "                                  transform=preprocess_transform_pretrain, batch_size=256, unseen=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embedding_loader = utils.generate_embedding_loader(TRAIN_DIR, \"train\", categories=categories, \n",
    "                                  transform=preprocess_transform_pretrain, batch_size=16, unseen=False)\n",
    "val_seen_embedding_loader = utils.generate_embedding_loader(VALID_IMG_DIR, \"val_seen\", categories=categories, \n",
    "                                  transform=preprocess_transform_pretrain, batch_size=16, unseen=False)\n",
    "val_unseen_embedding_loader = utils.generate_embedding_loader(VALID_IMG_DIR, \"val_unseen\", categories=categories, \n",
    "                                  transform=preprocess_transform_pretrain, batch_size=16, unseen=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_triplet_loader = utils.generate_triplet_loader(TRAIN_DIR, \"train\", categories=categories, \n",
    "                                  transform=preprocess_transform_pretrain, batch_size=64, unseen=False)\n",
    "val_triplet_loader = utils.generate_triplet_loader(VALID_IMG_DIR, \"val_seen\", categories=categories, \n",
    "                                  transform=preprocess_transform_pretrain, batch_size=64, unseen=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch Lightning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Siamese Sigmoid Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    for lr in [1e-5, 5e-5, 1e-4, 5e-4, 1e-3]:\n",
    "        callbacks = [\n",
    "            ModelCheckpoint(monitor='val_loss', save_top_k=1, dirpath=None),\n",
    "            EarlyStopping(monitor=\"val_loss\", min_delta=0.0, patience=10, verbose=False, mode=\"min\")\n",
    "            ]\n",
    "        logger = TensorBoardLogger(\"lightning_logs\", name=f\"Sigmoid-50-batchsize-256-lr-{str(lr)}\")\n",
    "        trainer = pl.Trainer(max_epochs=50, accelerator=\"gpu\", auto_lr_find=False, gradient_clip_val=0.0, log_every_n_steps=50, logger=logger, callbacks=callbacks)\n",
    "        model = models.SiameseSigmoid(lr=lr, pretrained=True)\n",
    "        combined_val_loader = model.val_dataloader(val_seen_loader, val_unseen_loader)\n",
    "        trainer.fit(model, train_loader, combined_val_loader)\n",
    "except:\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Siamese Cosine Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    for lr in [1e-5, 5e-5, 1e-4, 5e-4, 1e-3]:\n",
    "        callbacks = [\n",
    "            ModelCheckpoint(monitor='val_loss', save_top_k=1, dirpath=None),\n",
    "            EarlyStopping(monitor=\"val_loss\", min_delta=0.0, patience=10, verbose=False, mode=\"min\")\n",
    "            ]\n",
    "        logger = TensorBoardLogger(\"lightning_logs\", name=f\"Cosine-50-batchsize-256-no-pretrain-lr-{str(lr)}\")\n",
    "        trainer = pl.Trainer(max_epochs=50, accelerator=\"gpu\", auto_lr_find=False, gradient_clip_val=0.0, log_every_n_steps=50, logger=logger, callbacks=callbacks)\n",
    "        model = models.SiameseCosine(lr=lr, categories=categories, pretrained=True)\n",
    "        combined_val_loader = model.val_dataloader(val_seen_loader, val_unseen_loader)\n",
    "        trainer.fit(model, train_loader, combined_val_loader)\n",
    "except:\n",
    "    traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlmi12-mp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15 (main, Nov 24 2022, 08:28:41) \n[Clang 14.0.6 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1c47b866e51d790132ec72c2a9f3cf4c489e8988c6b8dabcc9077ea3087acb05"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
